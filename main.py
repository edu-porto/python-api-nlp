from flask import Flask, jsonify, request
import pickle
import nltk
from collections import Counter
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
import json
import string
from wordcloud import WordCloud
import matplotlib.pyplot as plt



# Abrindo o dicionÃ¡rio de dados que foram feitos o BoW
with open('dict_bow.pkl', 'rb') as dict_bow:
    dictionary_custom = pickle.load(dict_bow)

# Abrindo o modelo 
with open('modelo_naive_bayes.pkl', 'rb') as model_nb:
    model = pickle.load(model_nb)

# DicionÃ¡rios de emoji 
emoji_dict = {
    'ğŸ˜€': 'feliz ',
    'ğŸ˜‚': 'risos ',
    'ğŸ˜”': 'triste ',
    'ğŸ‘': 'palmas ',
    'ğŸ¥°': 'amÃ¡vel ',
    'ğŸ’™': 'coraÃ§Ã£o azul ',
    'ğŸ™ğŸ¼': 'orando ',
    'âœ¨': 'brilhando ',
    'ğŸ¤®': 'nojo ',
    'ğŸš€': 'foguete ',
    'ğŸ‘¿': 'diabo ',
    'ğŸ¤¢': 'nojo ',
    'ğŸ”¥': 'fogo ',
    'ğŸ˜¡': 'fÃºria ',
    'ğŸ˜ ': 'raiva ',
    'ğŸ¤£': 'rindo ',
    'ğŸ˜ƒ': 'feliz ',
    'ğŸ˜': 'curtindo ',
    'ğŸ˜Š': 'feliz ',
    'ğŸ¤©': 'maravilhado ',
    'ğŸ˜‹': 'delicioso ',
    'ğŸ˜†': 'risada ',
    'ğŸ˜Œ': 'calmo ',
    'ğŸ¤”': 'pensativo ',
    'ğŸ˜·': 'mÃ¡scara ',
    'ğŸ¤£': 'muitoRiso ',
    'ğŸ¥º': 'carinhoso ',
    'ğŸ‘': 'positivo ',
    'ğŸ¤¯': 'menteExplodida ',
    'ğŸ˜…': 'alÃ­vio ',
    'ğŸ¥°': 'carinhaComCoraÃ§Ã£o ',
    'ğŸ˜“': 'suor ',
    'ğŸ˜‘': 'tÃ©dio',
    'ğŸ¤«': 'silÃªncio',
    'ğŸ¤': 'apertoDeMÃ£os',
    'ğŸ˜Š': 'sorriso',
    'ğŸ˜': 'apaixonado',
    'ğŸ˜­': 'choro ',
    'ğŸ¤—': 'abraÃ§o ',
    'ğŸ‰': 'festa ',
    'ğŸ˜': 'descolado ',
    'ğŸ˜±': 'surpresa ',
    'ğŸ˜´': 'sono ',
    'ğŸ™Œ': 'celebraÃ§Ã£o ',
    'ğŸ¤”': 'pensativo ',
    'ğŸ˜˜': 'beijo ',
    'ğŸ¥³': 'festeiro ',
    'ğŸ™„': 'revirarOsOlhos ',
    'ğŸ˜Œ': 'alÃ­vio ',
    'ğŸ¤«': 'segredo ',
    'ğŸ˜‡': 'inocente ',
    'ğŸ˜‚': 'muitoEngraÃ§ado ',
    'ğŸ¤”': 'pensando ',
    'ğŸ˜´': 'sono ',
    'ğŸ¤ª': 'loucura ',
    'ğŸ˜¢': 'decepcionadoAliviado ',
    'ğŸ˜¬': 'nervoso ',
    'ğŸ˜Œ': 'alÃ­vio',
    'ğŸ˜”': 'triste ',
    'ğŸ˜': 'desapontado ',
    'ğŸ˜¢': 'choro ',
    'ğŸ˜­': 'chorando ',
    'ğŸ˜¡': 'raiva ',
    'ğŸ¤¯': 'mente explodida ',
    'ğŸ˜³': 'surpreso ',
    'ğŸ˜±': 'gritando ',
    'ğŸ˜¨': 'assustado ',
    'ğŸ˜´': 'sono ',
    'ğŸ¥±': 'bocejando ',
    'ğŸ¤¢': 'enjoado ',
    'ğŸ¤®': 'vomitando ',
    'ğŸ¤§': 'espirro ',
    'ğŸ¤’': 'doente ',
    'ğŸ¤•': 'machucado ',
    'ğŸ¤‘': 'dinheiro ',
}

# Array de stop_words customizada 
stop_words_custom = [
            '@', 'banco', 'btg', 'brg', 'pactual', 'btgpactual', 'pq', 'q', 'pra', 'vcs', 'vc', 'i', 'p', 'kkk', 'y', 'of',
            'n', 'a', 'Ã ', 'as', 'o', 'os', 'e', 'aos', 'do', 'das', 'dos', 'das', 'de', 'deles', 'dela', 'deles', 'delas',
            'para', 'que', 'em', 'algo', 'algum', 'alguma', 'alguns', 'algumas', 'aqui', 'aquele', 'aquela', 'aqueles',
            'aquelas', 'aqui', 'aquilo', 'cÃ¡', 'com', 'como', 'cada', 'coisa', 'daquele', 'daquela', 'daquilo', 'daqueles',
            'daquelas', 'desse', 'deste', 'dessa', 'desses', 'destes', 'destas', 'ele', 'eles', 'ela', 'elas', 'eu', 'nos',
            'nÃ³s', 'vocÃªs', 'voces', 'enquanto', 'era', 'estÃ¡', 'estamos', 'estÃ£o', 'estar', 'estarÃ¡', 'estive', 'estivemos',
            'estiver', 'estivera', 'estiveram', 'estivÃ©ramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem',
            'estivÃ©ssemos', 'estiveste', 'estivestes', 'estou', 'farÃ¡', 'farta', 'farto', 'fez', 'fim', 'foi', 'fomos',
            'for', 'fora', 'foram', 'fÃ´ramos', 'forem', 'formos', 'fosse', 'fossem', 'fÃ´ssemos', 'foste', 'fostes', 'fui',
            'fÃ´ssemos', 'hÃ¡', 'houve', 'hoje', 'isso', 'isto', 'jÃ¡', 'lÃ¡', 'lhe', 'lhes', 'lo', 'logo', 'mas', 'me', 'mesma',
            'mesmas', 'mesmo', 'mesmos', 'meu', 'meus', 'minha', 'minhas', 'na', 'no', 'nas', 'nos', 'naquela', 'naquelas',
            'naquele', 'naqueles', 'nem', 'nessa', 'nessas', 'nesse', 'nesses', 'nesta', 'nestas', 'neste', 'nestes',
            'ninguÃ©m', 'nosso', 'nossa', 'nossos', 'nossas', 'num', 'numa', 'outra', 'outras', 'outro', 'outros', 'pela',
            'pelo', 'perante', 'pois', 'ponto', 'pontos', 'por', 'porÃ©m', 'porque', 'porquÃª', 'prÃ³pria', 'prÃ³prio',
            'prÃ³prias', 'prÃ³prios', 'qual', 'quando', 'quanto', 'quantos', 'quantas', 'quÃª', 'quem', 'quer', 'quereis',
            'querem', 'queremas', 'quis', 'quisemos', 'quiser', 'quisera', 'quiseram', 'quisÃ©ramos', 'quiserem',
            'quisermos', 'quisÃ©sseis', 'quisÃ©ssemos', 'quiseste', 'quisestes', 'quiseste', 'quisestes', 'quizer',
            'quizeram', 'quizerem', 'quizermos', 'quizesse', 'quizessem', 'quizÃ©ssemos', 'sÃ£o', 'se', 'seja', 'sejam',
            'sejamos', 'sem', 'sendo', 'ser', 'serÃ¡', 'serÃ£o', 'serÃ¡', 'seriam', 'serÃ­amos', 'serias', 'serÃ­eis', 'sete',
            'seu', 'seus', 'sob', 'sobre', 'sois', 'sÃ³', 'somos', 'sou', 'sua', 'suas', 'tal', 'talvez', 'tambÃ©m', 'te',
            'tem', 'tÃªm', 'temos', 'tendes', 'tenha', 'tenham', 'tenhamos', 'tenho', 'tens', 'ter', 'terÃ¡', 'terÃ£o',
            'terÃ¡', 'teriam', 'terÃ­amos', 'terias', 'terÃ­eis', 'teu', 'teus', 'teve', 'tivemos', 'tiver', 'tivera',
            'tiveram', 'tivÃ©ramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivÃ©ssemos', 'tiveste', 'tivestes',
            'tiveste', 'tivestes', 'um', 'uma', 'umas', 'uns' ]

# Definindo novas vÃ¡riaveis para carregar o modelo e dict
dict_bow = dictionary_custom
model_bow = model

app = Flask(__name__)


# FunÃ§Ãµes para gerar a prediÃ§Ã£o 

# Converte emojis em palavras 
def emoji_to_word(textos):
    textos_processados = []
    for texto in textos:
        palavras = texto.split()
        texto_processado = []
        for palavra in palavras:
            if palavra in emoji_dict:
                texto_processado.append(emoji_dict[palavra])
            else:
                texto_processado.append(palavra)
        texto_processado = ' '.join(texto_processado)
        textos_processados.append(texto_processado)
    
    return textos_processados

# Tokeniza o texto
def processarTexto(textos):
    textos_processados = []
    for texto in textos:
        texto = texto.lower()
        tokens = word_tokenize(texto)
        stop_words = [
            '@', 'banco', 'btg', 'brg', 'pactual', 'btgpactual', 'pq', 'q', 'pra', 'vcs', 'vc', 'i', 'p', 'kkk', 'y', 'of',
            'n', 'a', 'Ã ', 'as', 'o', 'os', 'e', 'aos', 'do', 'das', 'dos', 'das', 'de', 'deles', 'dela', 'deles', 'delas',
            'para', 'que', 'em', 'algo', 'algum', 'alguma', 'alguns', 'algumas', 'aqui', 'aquele', 'aquela', 'aqueles',
            'aquelas', 'aqui', 'aquilo', 'cÃ¡', 'com', 'como', 'cada', 'coisa', 'daquele', 'daquela', 'daquilo', 'daqueles',
            'daquelas', 'desse', 'deste', 'dessa', 'desses', 'destes', 'destas', 'ele', 'eles', 'ela', 'elas', 'eu', 'nos',
            'nÃ³s', 'vocÃªs', 'voces', 'enquanto', 'era', 'estÃ¡', 'estamos', 'estÃ£o', 'estar', 'estarÃ¡', 'estive', 'estivemos',
            'estiver', 'estivera', 'estiveram', 'estivÃ©ramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem',
            'estivÃ©ssemos', 'estiveste', 'estivestes', 'estou', 'farÃ¡', 'farta', 'farto', 'fez', 'fim', 'foi', 'fomos',
            'for', 'fora', 'foram', 'fÃ´ramos', 'forem', 'formos', 'fosse', 'fossem', 'fÃ´ssemos', 'foste', 'fostes', 'fui',
            'fÃ´ssemos', 'hÃ¡', 'houve', 'hoje', 'isso', 'isto', 'jÃ¡', 'lÃ¡', 'lhe', 'lhes', 'lo', 'logo', 'mas', 'me', 'mesma',
            'mesmas', 'mesmo', 'mesmos', 'meu', 'meus', 'minha', 'minhas', 'na', 'no', 'nas', 'nos', 'naquela', 'naquelas',
            'naquele', 'naqueles', 'nem', 'nessa', 'nessas', 'nesse', 'nesses', 'nesta', 'nestas', 'neste', 'nestes',
            'ninguÃ©m', 'nosso', 'nossa', 'nossos', 'nossas', 'num', 'numa', 'outra', 'outras', 'outro', 'outros', 'pela',
            'pelo', 'perante', 'pois', 'ponto', 'pontos', 'por', 'porÃ©m', 'porque', 'porquÃª', 'prÃ³pria', 'prÃ³prio',
            'prÃ³prias', 'prÃ³prios', 'qual', 'quando', 'quanto', 'quantos', 'quantas', 'quÃª', 'quem', 'quer', 'quereis',
            'querem', 'queremas', 'quis', 'quisemos', 'quiser', 'quisera', 'quiseram', 'quisÃ©ramos', 'quiserem',
            'quisermos', 'quisÃ©sseis', 'quisÃ©ssemos', 'quiseste', 'quisestes', 'quiseste', 'quisestes', 'quizer',
            'quizeram', 'quizerem', 'quizermos', 'quizesse', 'quizessem', 'quizÃ©ssemos', 'sÃ£o', 'se', 'seja', 'sejam',
            'sejamos', 'sem', 'sendo', 'ser', 'serÃ¡', 'serÃ£o', 'serÃ¡', 'seriam', 'serÃ­amos', 'serias', 'serÃ­eis', 'sete',
            'seu', 'seus', 'sob', 'sobre', 'sois', 'sÃ³', 'somos', 'sou', 'sua', 'suas', 'tal', 'talvez', 'tambÃ©m', 'te',
            'tem', 'tÃªm', 'temos', 'tendes', 'tenha', 'tenham', 'tenhamos', 'tenho', 'tens', 'ter', 'terÃ¡', 'terÃ£o',
            'terÃ¡', 'teriam', 'terÃ­amos', 'terias', 'terÃ­eis', 'teu', 'teus', 'teve', 'tivemos', 'tiver', 'tivera',
            'tiveram', 'tivÃ©ramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivÃ©ssemos', 'tiveste', 'tivestes',
            'tiveste', 'tivestes', 'um', 'uma', 'umas', 'uns'
        ]
        tokens = [
            token for token in tokens if token not in stop_words and not token.startswith('@') and token.isalpha()
        ]
        textos_processados.append(tokens)
    return textos_processados

#  Vetoriza as frases com o dicionÃ¡rio do BoW
def vetorizar_frases(frases, dictionary):
    frases = [' '.join(tokens) for tokens in frases]
    vectorizer = CountVectorizer()
    vectorizer.set_params(vocabulary=dictionary)
    frases_vetorizadas = vectorizer.fit_transform(frases)
    return frases_vetorizadas

# Rotas da api 

@app.route('/classificar', methods=['POST'])
def classificar():
    dados = request.json
    # Aplique a funÃ§Ã£o emoji_to_word() aos dados do web scraping
    textos_entrada = dados["dados"]
    textos_processados = emoji_to_word(textos_entrada)
    textos_processados = processarTexto(textos_processados)
    frases_vetorizadas = vetorizar_frases(textos_processados, dict_bow)
    frases_vetorizadas = frases_vetorizadas.toarray()  # Converter para matriz densa
    predicoes = model_bow.predict(frases_vetorizadas)

    # Mapear valores numÃ©ricos para palavras correspondentes
    mapeamento_classes = {0: "negativo", 1: "neutro", 2: "positivo"}
    predicoes_palavras = [mapeamento_classes[predicao] for predicao in predicoes]

    return json.dumps(predicoes_palavras)

# Conta a proporÃ§Ã£o de quantos sentimentos hÃ¡ no json
@app.route('/proporcoes', methods=['POST'])
def proporcoes():
    predicoes_palavras = request.json
    predicoes_palavras = predicoes_palavras["dados"]

    # Mapear os sentimentos para os valores numÃ©ricos correspondentes
    mapeamento_sentimentos = {"negativo": 0, "neutro": 1, "positivo": 2}
    predicoes_numeros = [mapeamento_sentimentos[sentimento] for sentimento in predicoes_palavras]

    # Contar a ocorrÃªncia de cada sentimento
    proporcoes = Counter(predicoes_numeros)

    # Calcular as proporÃ§Ãµes
    total = len(predicoes_numeros)
    proporcoes = {sentimento: count/total for sentimento, count in proporcoes.items()}

    return json.dumps(proporcoes)

# Rota que retorna a nuvem de palavras 
@app.route('/nuvem-palavras', methods=['POST'])
def nuvem_palavras():
    dados = request.json["dados"]
    
    # Unir todos os textos em uma Ãºnica string
    texto_completo = ' '.join(dados)

    # Criar a nuvem de palavras
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_completo)

    # Plotar a nuvem de palavras
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.tight_layout()

    # Salvar a imagem da nuvem de palavras em um arquivo
    imagem_nuvem = 'nuvem_palavras.png'
    plt.savefig(imagem_nuvem)

    # Retornar o nome do arquivo da imagem
    return jsonify({"imagem_nuvem": imagem_nuvem})


# Retorna o top 10 palavras
@app.route('/top-palavras', methods=['POST'])
def top_palavras():
    dados = request.json["dados"]

    # Unir todos os textos em uma Ãºnica string
    texto_completo = ' '.join(dados)

    # Tokenizar o texto em palavras
    tokens = word_tokenize(texto_completo)

    # Remover stopwords das palavras tokenizadas
    stop_words = set(stop_words_custom )
    pontuacoes = set(string.punctuation)
    tokens_filtrados = [token for token in tokens if token.lower() not in stop_words and token.isalpha()]

    # Contar a ocorrÃªncia das palavras
    contagem_palavras = Counter(tokens_filtrados)

    # Obter as top 10 palavras mais frequentes
    top_palavras = contagem_palavras.most_common(10)

    # Retornar as top 10 palavras em formato JSON
    return jsonify({"top_palavras": top_palavras})

@app.route('/maiores-correlacoes', methods=['POST'])
def maiores_correlacoes():
    dados = request.json["dados"]

    # Aplicar a funÃ§Ã£o emoji_to_word aos dados
    dados_processados = emoji_to_word(dados)

    # Aplicar a funÃ§Ã£o processarTexto aos dados
    dados_processados = processarTexto(dados_processados)

    # Unir todos os textos em uma Ãºnica lista de palavras
    palavras = [palavra for texto in dados_processados for palavra in texto if palavra != 'nÃ£o']

    # Contar a frequÃªncia das palavras
    contagem_palavras = {}
    for palavra in palavras:
        if palavra in contagem_palavras:
            contagem_palavras[palavra] += 1
        else:
            contagem_palavras[palavra] = 1

    # Encontrar as palavras que mais aparecem na entrada
    palavras_mais_frequentes = sorted(contagem_palavras.items(), key=lambda x: x[1], reverse=True)[:20]

    # Criar um dicionÃ¡rio para armazenar as correlaÃ§Ãµes das palavras mais frequentes
    maiores_correlacoes = {}

    # Encontrar as duas palavras que mais aparecem junto a cada palavra mais frequente
    for palavra, _ in palavras_mais_frequentes:
        palavras_relacionadas = [palavra]
        palavras_relacionadas.extend(
            sorted([p for p in palavras if p != palavra and p in contagem_palavras],
                   key=lambda p: contagem_palavras[p],
                   reverse=True)[:2]
        )
        maiores_correlacoes[palavra] = palavras_relacionadas

    # Retornar as maiores correlaÃ§Ãµes em formato JSON
    return jsonify({"maiores_correlacoes": maiores_correlacoes})


@app.route('/ping', methods=['GET'])
def test():
    return ("Pong")


if __name__ == '__main__':
    app.run()
